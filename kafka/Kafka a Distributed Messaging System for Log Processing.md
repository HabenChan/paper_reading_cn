# Kafka: a Distributed Messaging System for Log Processing

## 摘要

日志处理已成为消费者互联网公司数据管道的关键组成部分。我们介绍的Kafka，一个我们针对大量日志收集和交付开发的分布式消息引擎系统。我们的系统结合了现有日志聚合器和消息引擎系统的想法，并适合实时消费和离线消费的场景。我们在Kafka做了很多非常规和实用的设计选择，以使我们的系统高效且可扩展，我们的实验结果表明，Kafka和两个流行的消息引擎系统相比具有更好的性能。我们在生产中使用Kafka已经有一段时间了，它每天处理数百GB的新数据。

## 关键词

消息传递、分布式、日志处理、吞吐量、在线实时

## 1.简介

任何一家规模可观的互联网公司都会产生大量的“日志”数据。这些数据通常包括1）用户活动与登录、页面浏览、点击、“喜欢”对应的事件，共享、评论和搜索查询；任何一家规模可观的互联公司都会产生大量的“日志”数据。这些数据通常包括(1)与登录、页面浏览量、点击、“喜欢”、分享、评论和搜索查询相关的用户活动事件;(2)操作指标，如服务调用堆栈、调用延迟、错误，以及系统指标，如每台机器上的CPU、内存、网络或磁盘利用率。长期以来，日志数据一直是分析的组成部分，用于跟踪用户参与度、系统利用率和其他指标。然而，最近的互联网应用趋势已经使活动数据成为直接用于站点特征的生产数据管道的一部分。这些用途包括(1)搜索相关性，(2)可能由项目流行程度或活动流中的共现驱动的建议，(3)广告目标和报告，以及(4)安全应用，防止滥用行为，如垃圾邮件或未经授权的数据抓取，还有新闻推送功能，可以聚合用户的状态更新或行为，供他们的“朋友”或“好友”阅读。这种日志数据的生产、实时使用给数据系统带来了新的挑战，因为它的容量比“真实”数据大几个数量级。例如，搜索、推荐和广告通常需要计算细粒度的点击率，它不仅为每个用户的点击生成日志记录，而且还为每个页面上未被点击的数十个条目生成日志记录。中国移动每天收集 5-8TB的通话记录[11]，Facebook收集近6TB的各种用户活动事件[12]。许多处理这类数据的早期系统依赖于从生产服务器上抓取日志文件进行分析。近年来，已经建立了几个专门的分布式日志 聚 合 器 ， 包 括 Facebook 的 Scribe[6] 、 雅 虎 的 DataHighway[4]和Cloudera的Flume[3]。这些系统主要用于收集日 志数据并将其加载到数据仓库或 Hadoop[8]中以供离线使用。在LinkedIn(一个社交网站)，我们发现除了传统的离线分析外，我们还需要支持上面提到的大多数实时应用程序，延迟不超过几秒钟。我们为日志处理构建了一个名为 Kafka[18]的新型消息传递系统，它结合了传统日志聚合器和消息传递系统的优点。一方面Kafka 是分布式和可伸缩的，并提供高吞吐量。另一方面，Kafka提供了一个类似于消息传递系统的API，允许应用程序实时使用日志事件。Kafka 已经开源，并在领英的产品中成功使用了 6 个多月。它极大地简化了我们的基础设施，因为我们可以利用单个软件来在线和离线地使用所有类型的日志数据。本文的其余部分组织如下。我们将在第 2 节中回顾传统的消息传递系统和日志聚合器。在第三节中，我们描述了 Kafka 的架构和它的关键设计原则。我们在第 4 节描述了我们在 LinkedIn 上部署Kafka，在第5节描述了Kafka的性能结果。我们将讨论未来的工作并在第6节进行总结。

## 2.相关工作

传统的企业消息传递系统[1][7][15][17]已经存在了很长一段时间，通常作为处理异步数据流的事件总线发挥着关键作用。但是，有几个原因可以解释为什么它们不适合用于日志处理。首先，企业系统提供的特性不匹配。这些系统通常专注于提供一套丰富的交付保证。例如，IBM Websphere MQ[7]具有事务性支持，允许应用程序以原子方式将消息插入多个队列。JMS[14]规范允许在使用后确认每个单独的消息，这可能是无序的。这样的交付保证对于收集日志数据通常是多余的。例如，偶尔失去一些浏览量当然不是世界末日。这些不需要的特性往往会增加API和这些系统的底层实现的复杂性。其次，许多系统并不像它们的主要设计约束那样强烈地关注吞吐量。例如，JMS没有API允许生产者显式地将多个消息批处理到单个请求。这意味着每个消息都需要一个完整的TCP/IP 往返，这对于我们的场景的吞吐量需求来说是不可行的。第三，这些系统在分布式支持方面很薄弱。在多台机器上分区和存储消息没有简单的方法。最后，许多消息传递系统假定消息几乎立即被使用，因此未被使用的消息队列总是相当小。如果允许消息累积，它们的性能会显著下降，就像离线消费者(如定期进行大量 负载而不是持续消费的数据仓库应用程序)的情况一样。在过去的几年中，已经构建了许多专门的日志聚合器。Facebook使用一个名为 Scribe的系统。每个前端机器可以通过套接字向一组Scribe机器发送日志数据。每个 Scribe机器聚合日志条目，并定期将其转储到 HDFS[9]或 NFS 设备。雅虎的数据高速公路项目也有类似的数据流。一组机器聚合来自客户机的事件，并输出“分钟”文件，然后将这些文件添加到 HDFS。Flume 是Cloudera开发的一个相对较新的日志聚合器。它支持可扩展的“管道”和“接收器”，并使流日志数据非常灵活。它还具有更集成的分布式支持。然而，这些系统中的大多数都是为离线使用日志数据而构建的，并且经常向使用者公开不必要的实现细节(例如“分钟文件”)。此外，它们大多数使用“推”模型，在这种模型中，Broker将数据转发给消费者。在 LinkedIn，我们发现“拉”模型更适合我们的应用程序，因为每个消费者都可以以最大的速度获取信息，并避免被超过其处理速度的信息淹没。拉动模型也使消费者很容易倒带，我们将在第 3.2节的末尾讨论这个好处。 最近，雅虎!研究人员开发了一种新的分布式 pub/sub 系统，名为 HedWig[13]。HedWig 是高度可扩展和可用的，并提供强大的耐久性保证。但是，它主要用于存储数据存储的提交日志。

## 3.Kakfa设计架构与原则

由于现有系统的局限性，我们开发了一个新的基于消息的日志聚合器Kafka。我们首先介绍Kafka的基本概念。特定类型的消息流由主题定义。生产者可以向主题发布消息。发布的消息然后存储在一组称为Broker的服务器上。使用者可以从Broker订阅一个或多个主题，并通过从Broker提取数据来使用订阅的 消息。消息传递在概念上是简单的，我们已经尝试让 Kafka API同样简单地反映这一点。我们没有展示确切的 API，而是展示了一些示例代码来展示如何使用 API。生成器的示例代码如下所示。消息被定义为只包含字节的有效负载。用户可以选择自己喜欢的序列化方法来对消息进行编码。为了提高效率，生产者可以在一个发布请求中发送一组消息。

```text
Sample producer code:
producer = new Producer(…);
message = new Message(“test message str”.getBytes());
set = new MessageSet(message);
producer.send(“topic1”, set);
```

要订阅主题，使用者首先为该主题创建一个或多个消息流。消息被发布到主题后，将均匀分布到这些子流中。关于 Kafka 如何分发消息的细节将在 3.2 节后面描述。每个消息流在生成的连续消息流上提 供一个迭代器接口。然后使用者遍历流中的每条消息，并处理消息的有效负载。与传统的迭代器不同，消息流迭代器永远不会终止。如果当前没有更多消息可供使用，迭代器将阻塞，直到将新的消息发布到主题。我们既支持点到点传递模型(多个使用者共同使用主题中所有消息的单个副本)，也支持发布/订阅模型(多个使用者分别检索主题的自己的副本)。

Sample consumer code:

```java
streams[] = Consumer.createMessageStreams(“topic1”, 1)
for (message : streams[0]) {
bytes = message.payload();
// do something with the bytes
}
```

Kafka 的整体架构如图 1所示。由于Kafka 本质上是分布式的，一个Kafka 集群通常由多个 broker 组成。为了平衡负载，一个主题被划分为多个分区，每个Broker存储一个或多个这些分区。多个生产者和消费者可以同时发布和检索消息。在3.1节中，我们将描述Broker上单个分区的布局，以及为了提高访问分区的效率而选择的一些设计选项。在第3.2节中，我们描述了生产者和消费者如何在分布式设置中与多个Broker交互。我们在第3.3节讨论了Kafka的交付保证。

![img](https://pic2.zhimg.com/80/v2-46c181cdd0e342dcaf23a1ca5a5a88cd_720w.webp)



3.1 **单个分区的效率**

我们在Kafka 中做了一些决策来提高系统的效率。 **Simple Storage**:Kafka 有一个非常简单的存储布局。主题的每个分区对 应一个逻辑日志。从物理上讲，日志是由一组大约相同大小(例如，1GB)的段文件实现的。每次生产者向分区发布消息时，Broker只会将消息追加到最后一个段文件。为了获得更好的性能，只有在发布了可配置数量的消息或经过一定时间后，才将段文件刷新到磁盘。消息只有在刷新后才向使用者公开。

与典型的消息系统不同，存储在 Kafka中的消息没有一个显式的消息 id。相反，每个消息都是通过日志中的逻辑偏移来寻址的。这避免了维护辅助的、搜索密集型的随机访问索引结构的开销，这些索引结构将消息 id 映射到实际的消息位置。注意，我们的消息 id 在增加，但不是连续的。为了计算下一 条消息的 id，我们必须将当前消息的长度添加到其 id中。从现在开始，我们将交替使用消息id 和偏移量。

使用者总是按顺序消费来自特定分区的消息。如果消费者确认了一个特定的消息偏移量，这意味着消费者已经收到了该分区中在该偏移量之前的所有消息。在表面之下，使用者向Broker发出异步拉取请求，以便有一个数据缓冲区供应用程序使用。每个 pull 请求包含消费开始时消息的偏移量和可接受的字节数。每个Broker在内存中保存一个已排序的偏移量列表，包括每个段文件中第一个消息的偏移量。Broker通过搜索偏移量列表来定位所请求消息所在的段文件，并将数据发送回消费者。消费者收 到消息后，它会计算下一个消息的偏移量，并在下一个 pull 请求中使用它。Kafka日志和内存索引的布局如图2所示。每个框显示消息的偏移量。

![img](https://pic4.zhimg.com/80/v2-38a10ff347565c4e7bc888721d4eab3b_720w.webp)

高效传输:我们在传输进出Kafka 的数据时非常小心。在前面，我们已经展示了生产者可以在一个发送请求中提交一组消息。尽管最终消费者 API每次只迭代一条消息，但在表面之下，来自消费者的每个拉取请求也会检索到多个大小为一定大小的消息，通常为数百千字节。

我们做的另一个非传统的选择是避免在 Kafka 层显式缓存消息。相反，我们依赖底层的文件系统页面缓存。这样做的主要好处是避免了双缓冲——消息只缓存在页面缓存中。这样做的另一个好处是即使在Broker进程重新启动时也能保留热缓存。由于Kafka 根本不缓存进程中的消息，它在垃圾回收内存方面的开销非常小，使得在基于虚拟机的语言中高效实现成为可能。最后，因为生产者和消费者都访问段文件。

接下来，由于使用者通常会比生产者滞后一小部分，所以正常的操作系统缓存启发式是非常有效的(特别是透写缓存和提前读)。我们发现，产品和消费都具有与数据大小线性一致的性能，最高可达许多TB的数据。

此外，我们为消费者优化网络接入。Kafka是一个多用户系统，一条消息可以被不同的消费者应用多次使用。将字节从本地文件发送到远程套接字的典型方法包括以下步骤:

(1)从存储介质中读取数据到操作系统中的页缓存，

(2)将页缓存中的数据复制到应用程序缓冲区，

(3)将应用程序缓冲区复制到另一个内核缓冲区，

(4)将内核缓冲区发送到套接字。这包括 4 个数据复制和 2个系统调用。

在 Linux 和其他 Unix 操作系统上，存在一个sendfile API[5]，它可以直接将字节从文件通道传输到套接字通道。这通常避免了步骤(2)和步骤(3)中引入的 2个副本和 1个系统调用。Kafka 利用 sendfile API 高效地将日志段文件中的字节从broker传递给消费者。

**Stateless broker**:与其他大多数消息系统不同，在 Kafka中，关于每个消费者消费了多少的信息不是由Broker维护的，而是由消费者自己维护的。这样的设计大大降低了Broker的复杂性和开销。然而，这使 得删除消息变得棘手，因为Broker不知道是否所有订阅者都使用了该消息。Kafka 通过使用一个简单的基于时间的SLA来解决这个问题。如果消息在Broker中保留的时间超过一定期限(通常为 7 天)，则该消息将被自动删除。这个解决方案在实践中效果很好。大多数消费者，包括线下消费者，每天、每小时或实时消费。事实上，Kafka 的性能不会随着更大的数据量而降低，这使得这种长时间的保留是可行的。 这种设计有一个重要的附带好处。用户可以故意倒回旧的偏移量并重新使用数据。这违反了队列的通用约定，但事实证明这是许多消费者的基本特性。例如，当消费者中的应用程序逻辑出现错误时，应用程序可以在错误修复后重新播放某些消息。这对于 ETL 数据加载到我们的数据仓库或 Hadoop 系统非常重要。另一个例子是，使用的数据可能只会周期性地刷新到持久存储中(例如，全文索引器)。如果使用者崩溃，未刷新的数据将丢失。在这种情况下，使用者可以对未刷新消息的最小偏移量 进行检查点，并在消息重启时重新使用该偏移量。我们注意到，拉模型比推模型更容易支持消费者。

## 3.2 分布式协调

现在我们将描述生产者和消费者在分布式环境中的行为。每个生产者可以将消息发布到随机选择的分区或由分区键和分区函数在语义上确定的分区。我们将重点关注消费者如何与Broker交互。

Kafka 有消费者群体的概念。每个消费者组由一个或多个消费者组成，这些消费者共同使用一组订阅的主题，也就是说，每条消息只传递给组内的一个消费者。不同的消费者组各自独立地使用完整的订阅消息集，不需要跨消费者组进行协调。消费者在同一个组中可以处于不同的进程或不同的机器上。我们的目标是将存储在Broker中的消息平均地分配给消费者，而不会引入太多的协调开销。

我们的第一个决定是使主题中的分区成为并行性的最小单元。这意味着在任何给定的时间，来自一个分区的所有消息只被每个消费者组中的单个消费者使用。如果我们允许多个使用者同时使用一个分区，它们就必须协调谁使用哪些消息，这就需要锁定和状态维护开销。相反，在我们的设计中，消费过程只需要在消费者重新平衡负载时进行协调，这是一个很少发生的事件。为了实现负载的真正平衡，我们在一个主题中需要比每个组中的用户更多的分区。通过对主题进行过度划分，我们可以很容易地实现这一点。

我们做出的第二个决定是不设置一个中央“主”节点，而是让消费者以一种去中心化的方式相互协调。添加主服务器可能会使系统复杂化，因为我们必须进一步担心主服务器故障。为了促进协调 ，我们 采 用 了 一 个 高 度 可 用 的 共 识 服 务Zookeeper[10]。Zookeeper 有一个非常简单的文件系统，比如API。可以创建路径，设置路径值，读取路径值，删除路径，列出路径的子节点。它做了一些更有趣的事情:(a)一个人可以注册一个路径上的观察者，当路径的子路径或路径的值发生变化时得到通知;(b)一个路径可以被创建为 ephemeral(相对于persistent)，这意味着如果创建的客户端消失，该路径会被Zookeeper 服务器自动删除;(c) zookeeper 将数据复制到多个服务器上，使数据具有高可靠性和可用性。

Kafka 使用 Zookeeper 完成以下任务 :(1) 检测 broker 和consumer 的添加和删除，(2)当上述事件发生时，在每个consumer 中触发 rebalance 进程，(3)维护消费关系，并跟踪每个 partition 的消费偏移量。具体来说，当每个Broker或消费者启动时，它将其信息存储在 Zookeeper 中的Broker或消费者注册 表中。Broker注册表包含Broker的主机名和端口，以及存储在其中的主题集和分区。使用者注册表包括使用者所属的使用者组及其订阅的主题集。每个消费组都与 Zookeeper中的一个所有权注册表和一个偏移注册表相关联。所有权注册中心对每个订阅的分区都有一个路径，路径值是当前从该分区消费的消费者的 id(我们使用消费者拥有该分区的术语)。偏移量注册表为每个订阅的分区存储最后使用的消息在分区中的偏移量。

在Zookeeper 中创建的路径对于Broker注册表、消费者注册表和所有权注册表来说是短暂的，而对于偏移注册表来说是持久的。如果Broker出现故障，则将自动从Broker注册中心删除其上的所有分区。使用者的失败将导致它丢失其在使用者注册表中的条目以及在所有权注册表中所拥有的所有分区。每个消费者在Broker注册表和消费者注册表上都注册了一个 Zookeeper watchdog，当Broker组或消费者组发生变化时，会收到通知。

在消费者初始启动期间，或者当消费者通过观察者得知Broker/消费者的更改时，消费者启动一个rebalance过程来确定新的

![img](https://pic1.zhimg.com/80/v2-53f6e9dc8a6ec8941113eda2cc17e980_720w.webp)

当一个组中有多个消费者时，每一个消费者都会收到Broker或消费者更改的通知。但是，通知可能会在不同的时间出现在消费者那里。因此，有可能一个消费者试图获得分区的所有权，该分区仍然属于另一个消费者。当发生这种情况时，第一个使用者只需释放它当前拥有的所有分区，稍等片刻，然后重试 rebalance 过程。在实践中，再平衡过程通常在几次尝试后就稳定下来了。当创建一个新的消费组时，在偏移量注册表中没有可用的偏移量。在这种情况下，消费者将使用我们在Broker上提供的API，从每个订阅分区上可用的最小或最大偏移量(取决于配置)开始。

## 3.3 交付保证

一般来说，Kafka 只保证至少一次的交付。正是如此——一次交付通常需要两阶段提交，对于我们的应用程序来说并不是必需的。大多数情况下，消息只向每个消费者组发送一次。然而，当一个消费进程在没有完全关闭的情况下崩溃时，接管失败的消费进程拥有的那些分区的消费进程可能会在最后一个偏移量成功提交给 zookeeper 后得到一些重复的消息。如果应用程序关心重复数据，它必须添加自己的重复数据删除逻辑，要么使用我们返回给消费者的偏移量，要么使用消息中的某个唯一键。这通常是一种比使用两阶段提交更具成本效益的方法。

为了避免日志损坏，Kafka 为日志中的每条消息存储了一个CRC。如果 broker 上有任何 I/O 错误，Kafka 会运行一个恢复进程来删除那些 crc 不一致的消息。在消息级别使用 CRC还允许我们在产生或使用消息后检查网络错误。如果Broker故障，存储在其上的任何尚未使用的消息都将不可用。如果Broker上的存储系统永久损坏，则任何未使用的消息将永远丢失。在未来，我们计划在 Kafka 中添加内置的复制，以在多个Broker上冗余地存储每个消息。

## 4..在LinkedIn上使用Kafka

在本节中，我们将介绍如何在 LinkedIn 上使用 Kafka。图 3显示了我们的部署的简化版本。我们在每个数据中心都有一个 Kafka集群，在那里我们运行面向用户的服务。前端服务生成各种各样的日志数据，并将其批量发布到本地 Kafka broker。我们依靠一个硬件负载均衡器将发布请求均匀地分发到 Kafka Broker集合。Kafka的在线消费者运行在同一个数据中心的服务中。

![img](https://pic1.zhimg.com/80/v2-b6436f0fae272d17f8d752b0b50a6518_720w.webp)

我们还在一个独立的数据中心部署了一个 Kafka 集群来进行离线分析，地理位置靠近我们的 Hadoop 集群和其他数据仓库基础设施。这个Kafka 实例运行一组嵌入式消费者，从实时数据中心的 Kafka 实例中提取数据。然后我们运行数据加载任务将数据从这个 Kafka 的复制集群拉到 Hadoop 和我们的数据仓库，在那里我们运行各种报告任务和数据分析过程。我们也使用这个Kafka 集群来创建原型，并且能够针对原始事件流运行简单的脚本来进行特别的查询。在不进行太多调优的情况下，整个管道的端到端延迟平均约为10秒，足以满足我们的需求。目前，Kafka每天积累了数百 gb的数据和接近 10亿条消息，我们预计随着我们完成对遗留系统的转换以利用 Kafka，这一数据将显著增长。未来还将添加更多类型的消息。再平衡过程能够自动重定向当操作人员启动或停止中间商进行软硬件维护时消耗。

我们的跟踪还包括一个审计系统，以验证在整个管道中没有数据丢失。为了方便这一点，每条消息在生成时都带有时间戳和服务器名。我们检测每个生产者，使其周期性地生成一个监视事件，该事件记录了生产者在固定时间窗口内为每个主题发布的消息数量。生产者在一个单独的主题中发布监控事件到Kafka。然后，使用者可以计算从给定主题接收到的消息的数量，并使用监视事件验证这些计数，以验证数据的正确性。加载到Hadoop 集群是通过实现一种特殊的 Kafka 输入格式来完成的，这种格式允许 MapReduce 任务直接从 Kafka 读取数据。MapReduce 任务加载原始数据，然后对其进行分组和压缩，以便将来进行高效的处理。在这里，消息偏移量的无状态Broker和客户端存储再次发挥作用，允许MapReduce 任务管理(它允许任务失败并重新启动)以自然的方式处理数据加载，而不会在任务重新启动时复制或丢失消息。只有在任务成功完成时，数据和偏移量才会存储在HDFS中。我们选择使用Avro[2]作为我们的序列化协议，因为它是高效的，并且支持模式演化。对于每个消息，我们在负载中存储其Avro模式的 id 和序列化的字节。这个模式允许我们执行契约，以确保数据生产者和消费者之间的兼容性。我们使用轻量级模式注册中心服务将模式 id 映射到实际的模式。当使用者获得消息时，它在模式注册表中查找以检索模式，该模式用于将字节解码为对象(由于值是不可变的，因此每个模式只需要进行一次查找)。

## 5.结论和未来工作

我们提出了一个名为 Kafka 的新系统来处理大量的日志数据流。像一个消息传递系统，Kafka 采用了一个基于拉动的消费模型，允许应用程序以自己的速率消费数据，并在需要的时候倒带消费。通过专注于日志处理应用程序，Kafka 实现了比传统消息系统更高的吞吐量。它还提供了集成的分布式支持，并且可以向外扩展。我们在 LinkedIn 成功地将 Kafka用于离线和在线应用。未来我们有很多方向要追求。首先，我们计划在多个Broker之间添加内置的消息复制，以保证持久性和数据可用性，即使在不可恢复的机器故障情况下也是如此。我们希望同时支持异步和同步复制模型，以便在生产者延迟和所提供的保证的强度之间进行一些权衡。应用程序可以根据持久性、可用性和吞吐量的需求选择合适的冗余级别。其次，我们想在 Kafka 中添加一些流处理能力。从Kafka 中获取消息后，实时应用程序通常会执行类似的操作，比如基于窗口的计数，以及将每个消息与二级存储中的记录或另一个流中的消息连接起来。在最低级别上，这是由发布期间对连接键进行语义分区的消息支持的，这样，使用特定键发送的所有消息都将进入同一个分区，从而到达单个消费者进程。这为跨消费机器集群处理分布式流提供了基础。在此之上，我们认为一个有用的流实用程序库，如不同的窗口函数或连接技术将有利于这类应用程序。